{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using CNTK backend\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND']='cntk'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers import merge\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", string)    \n",
    "    string = re.sub(r\"\\'\", \"\", string)    \n",
    "    string = re.sub(r\"\\\"\", \"\", string)    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"C:/Users/pranjal/Desktop/kettle/model_data/\"\n",
    "SCIENCE_PATH = BASE_PATH + \"science/*.txt\"\n",
    "TECH_PATH = BASE_PATH + \"technology/*.txt\"\n",
    "science_files = glob.glob(SCIENCE_PATH)\n",
    "tech_files = glob.glob(TECH_PATH)\n",
    "science_file_list = []\n",
    "tech_file_list = []\n",
    "for file_name in science_files:\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        science_file_list.append(\" \".join(f.readlines()))\n",
    "\n",
    "for file_name in tech_files:\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        tech_file_list.append(\" \".join(f.readlines()))\n",
    "        \n",
    "X = science_file_list + tech_file_list\n",
    "y = ([0] * len(science_file_list)) + ([1] * len(tech_file_list))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = X_train\n",
    "labels = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\keras_preprocessing\\text.py:174: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48880 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape of data tensor:', (879L, 1000L))\n",
      "('Shape of label tensor:', (879L, 2L))\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive and negative reviews in traing and validation set \n",
      "[384. 320.]\n",
      "[93. 82.]\n"
     ]
    }
   ],
   "source": [
    "print('Number of positive and negative reviews in traing and validation set ')\n",
    "print y_train.sum(axis=0)\n",
    "print y_val.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_cov1= Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "l_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\n",
    "l_pool2 = MaxPooling1D(5)(l_cov2)\n",
    "l_cov3 = Conv1D(128, 5, activation='relu')(l_pool2)\n",
    "l_pool3 = MaxPooling1D(35)(l_cov3)  # global max pooling\n",
    "l_flat = Flatten()(l_pool3)\n",
    "l_dense = Dense(128, activation='relu')(l_flat)\n",
    "preds = Dense(2, activation='softmax')(l_dense)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 704 samples, validate on 175 samples\n",
      "Epoch 1/20\n",
      "704/704 [==============================] - 6s 8ms/step - loss: 0.9895 - acc: 0.5312 - val_loss: 0.6819 - val_acc: 0.5314\n",
      "Epoch 2/20\n",
      "704/704 [==============================] - 3s 4ms/step - loss: 0.6872 - acc: 0.5611 - val_loss: 0.6594 - val_acc: 0.5429\n",
      "Epoch 3/20\n",
      "704/704 [==============================] - 3s 4ms/step - loss: 0.6406 - acc: 0.6335 - val_loss: 0.6072 - val_acc: 0.6971\n",
      "Epoch 4/20\n",
      "704/704 [==============================] - 3s 4ms/step - loss: 0.6694 - acc: 0.6264 - val_loss: 0.6463 - val_acc: 0.5543\n",
      "Epoch 5/20\n",
      "704/704 [==============================] - 3s 4ms/step - loss: 0.5942 - acc: 0.6804 - val_loss: 0.6224 - val_acc: 0.8114\n",
      "Epoch 6/20\n",
      "704/704 [==============================] - 3s 4ms/step - loss: 0.5997 - acc: 0.7287 - val_loss: 0.5603 - val_acc: 0.7143\n",
      "Epoch 7/20\n",
      "704/704 [==============================] - 3s 4ms/step - loss: 0.5332 - acc: 0.7486 - val_loss: 0.4821 - val_acc: 0.8057\n",
      "Epoch 8/20\n",
      "704/704 [==============================] - 3s 4ms/step - loss: 0.5126 - acc: 0.7685 - val_loss: 0.4696 - val_acc: 0.7486\n",
      "Epoch 9/20\n",
      "704/704 [==============================] - 3s 4ms/step - loss: 0.4198 - acc: 0.8281 - val_loss: 0.5182 - val_acc: 0.8000\n",
      "Epoch 10/20\n",
      "704/704 [==============================] - 3s 4ms/step - loss: 0.4325 - acc: 0.8125 - val_loss: 0.3689 - val_acc: 0.8457\n",
      "Epoch 11/20\n",
      "704/704 [==============================] - 3s 4ms/step - loss: 0.4451 - acc: 0.7784 - val_loss: 0.3835 - val_acc: 0.8457\n",
      "Epoch 12/20\n",
      "704/704 [==============================] - 3s 4ms/step - loss: 0.3619 - acc: 0.8636 - val_loss: 0.3470 - val_acc: 0.8514\n",
      "Epoch 13/20\n",
      "704/704 [==============================] - 3s 4ms/step - loss: 0.3258 - acc: 0.8651 - val_loss: 0.3306 - val_acc: 0.8457\n",
      "Epoch 14/20\n",
      "704/704 [==============================] - 3s 4ms/step - loss: 0.4552 - acc: 0.7841 - val_loss: 0.3547 - val_acc: 0.8571\n",
      "Epoch 15/20\n",
      "704/704 [==============================] - 3s 4ms/step - loss: 0.2901 - acc: 0.8750 - val_loss: 0.4127 - val_acc: 0.8514\n",
      "Epoch 16/20\n",
      "704/704 [==============================] - 3s 4ms/step - loss: 0.3243 - acc: 0.8849 - val_loss: 0.4814 - val_acc: 0.8171\n",
      "Epoch 17/20\n",
      "704/704 [==============================] - 3s 4ms/step - loss: 0.3404 - acc: 0.8693 - val_loss: 0.3241 - val_acc: 0.8686\n",
      "Epoch 18/20\n",
      "704/704 [==============================] - 3s 4ms/step - loss: 0.2770 - acc: 0.9119 - val_loss: 0.3603 - val_acc: 0.8514\n",
      "Epoch 19/20\n",
      "704/704 [==============================] - 3s 4ms/step - loss: 0.2557 - acc: 0.9148 - val_loss: 0.3274 - val_acc: 0.8571\n",
      "Epoch 20/20\n",
      "704/704 [==============================] - 3s 4ms/step - loss: 0.2655 - acc: 0.9034 - val_loss: 0.5340 - val_acc: 0.6686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xbe7f4a8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          nb_epoch=20, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "652 704 0.926136363636\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x_train)\n",
    "cnt = 0\n",
    "for i in range(0, len(pred)):\n",
    "    if pred[i][0] >= 0.3 and y_train[i][0] == 1:\n",
    "        cnt += 1\n",
    "    elif pred[i][1] >= 0.3 and y_train[i][1] == 1:\n",
    "        cnt += 1\n",
    "\n",
    "print cnt, len(pred), float(cnt) / len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = []\n",
    "y_model = []\n",
    "for item in y_train:\n",
    "    if item[0] == 1:\n",
    "        y_actual.append(0)\n",
    "    else:\n",
    "        y_actual.append(1)\n",
    "for item in pred:\n",
    "    y_model.append(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9876627604166667"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_actual, y_model, pos_label=0)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 14, 5]\n"
     ]
    }
   ],
   "source": [
    "x=[1,2,3]\n",
    "y=[14,5]\n",
    "print x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
